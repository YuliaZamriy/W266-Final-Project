{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet tensorflow-hub\n",
    "# !pip install --quiet seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0401 15:43:00.600650 140262053664512 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "import re\n",
    "import itertools\n",
    "from importlib import reload\n",
    "\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_data\n",
    "from helpers import explore_data\n",
    "from helpers import preprocess_data\n",
    "from helpers import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0401 15:44:10.311100 140262053664512 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_hub.module.Module"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/tf/notebooks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = main_dir + 'final-project/data/sample/hein-daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File speeches_097.txt has 249718612 characters\n",
      "and 283399 speeches\n",
      "\n",
      "Speeches list has 283399 speeches\n"
     ]
    }
   ],
   "source": [
    "all_speech_ids, all_speeches = load_data.load_speech_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2914465 lines have been read\n",
      "37617 keys had duplicates and deleted\n",
      "The dictionary has 2808050 keys\n",
      "\n",
      "Random congressperson: 1090105359\n",
      "Party D\n",
      "Chamber H\n",
      "Ethnicity W\n",
      "State NY\n",
      "Age 77.0\n",
      "speakerid 109121410.0\n",
      "speaker Ms. SLAUGHTER\n",
      "Full Name SLAUGHTER, LOUISE\n",
      "NonWhite 0.0\n",
      "Age_lt_med 0.0\n",
      "date 20060201\n",
      "match exact\n",
      "word_count 12\n",
      "check 1\n",
      "Female 1.0\n",
      "Congress 109\n",
      "char_count 59\n",
      "\n",
      "It took 16.8 seconds to create the dictionary\n"
     ]
    }
   ],
   "source": [
    "reload(load_data)\n",
    "start_time = time.time()\n",
    "descr = load_data.load_descr_data(main_dir+'final-project/data/QA/full_descr.txt')\n",
    "print(\"\\nIt took {0:.1f} seconds to create the dictionary\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283399"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender = load_data.create_target_labels(all_speech_ids, descr)[0]\n",
    "len(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training split: 1622 ones and 1622 zeroes\n",
      "Training speech list size: 3244\n",
      "Training target list size: 3244\n",
      "Validation split: 540 ones and 540 zeroes\n",
      "Validation speech list size: 1080\n",
      "Validation target list size: 1080\n",
      "Test split: 542 ones and 542 zeroes\n",
      "Test speech list size: 1084\n",
      "Test target list size: 1084\n"
     ]
    }
   ],
   "source": [
    "train, train_ids, train_target, val, val_ids, val_target, test, test_ids, test_target = preprocess_data.split_train_val_test(all_speeches, all_speech_ids, gender, descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data has 3244 speeches\n",
      "It was split into 37989 chunks\n",
      "Checks on ids and target 37989 37989\n",
      "Original target mean 0.5\n",
      "New target mean 0.5066203374661086\n"
     ]
    }
   ],
   "source": [
    "reload(preprocess_data)\n",
    "train_chunk, train_ids_chunk, train_target_chunk = preprocess_data.split_speech_to_chunks(train, train_ids, train_target, max_len=chunk_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data has 1080 speeches\n",
      "It was split into 12903 chunks\n",
      "Checks on ids and target 12903 12903\n",
      "Original target mean 0.5\n",
      "New target mean 0.4755483220956367\n"
     ]
    }
   ],
   "source": [
    "val_chunk, val_ids_chunk, val_target_chunk = preprocess_data.split_speech_to_chunks(val, val_ids, val_target, max_len=chunk_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Speaker. I want to compliment the gentleman from California for his comments. I think one of the things that he and the gentleman from Wisconsin were pointing out is',\n",
       " 'that they had some perfecting punendments to the Commission that would have made it better. I think some of the things that people worried about were the calendar days rather']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It took 40.5 seconds to create embeddings\n"
     ]
    }
   ],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    train_embeddings = session.run(embed(train_chunk))\n",
    "print(\"\\nIt took {0:.1f} seconds to create embeddings\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37989, 512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It took 19.4 seconds to create embeddings\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    val_embeddings = session.run(embed(val_chunk))\n",
    "print(\"\\nIt took {0:.1f} seconds to create embeddings\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12903, 512)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_path = main_dir + 'final-project/Classification/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(outdata_path, 'train_emb30'), 'wb') as fp:\n",
    "    pickle.dump(train_embeddings, fp)\n",
    "with open(os.path.join(outdata_path, 'val_emb30'), 'wb') as fp:\n",
    "    pickle.dump(val_embeddings, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(outdata_path, 'train_emb30'), 'rb') as fp:\n",
    "    train_embeddings = pickle.load(fp)\n",
    "with open(os.path.join(outdata_path, 'val_emb30'), 'rb') as fp:\n",
    "    val_embeddings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37989, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_embeddings).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_chunked_speech(data_emb, ids, ids_chunk, target, max_chunks=25):\n",
    "    \n",
    "    id_dict = {k:[] for k in ids} \n",
    "    for i in range(len(ids_chunk)):\n",
    "        id_dict[ids_chunk[i]].append(data_emb[i]) \n",
    "        \n",
    "    n_chunks = max_chunks\n",
    "    data_list, new_ids = [], []\n",
    "    for speech_id in id_dict:\n",
    "        new_ids.append(speech_id)\n",
    "        chunks = id_dict[speech_id].copy()\n",
    "        if len(chunks) < n_chunks:\n",
    "            padding = (n_chunks - len(id_dict[speech_id])) * [[0]*512]\n",
    "            chunks.extend(padding)\n",
    "            data_list.append(chunks)\n",
    "        else:\n",
    "            data_list.append(chunks[:n_chunks])    \n",
    "            \n",
    "    data_list_flat = list(itertools.chain.from_iterable(data_list))\n",
    "    data_array = np.reshape(np.array(data_list_flat), [len(ids), -1, 512])\n",
    "    print(\"Input data shape:\",data_array.shape)\n",
    "    \n",
    "    new_target = [target[ids.index(new_ids[i])] for i in range(len(ids))]\n",
    "    print(\"Target shape\", len(new_target))\n",
    "    \n",
    "    return data_array, new_target, new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (3244, 25, 512)\n",
      "Target shape 3244\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_target_emb, train_ids_emb = reshape_chunked_speech(train_embeddings, train_ids, train_ids_chunk, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (1080, 25, 512)\n",
      "Target shape 1080\n"
     ]
    }
   ],
   "source": [
    "val_emb, val_target_emb, val_ids_emb = reshape_chunked_speech(val_embeddings, val_ids, val_ids_chunk, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_base_dir = main_dir+\"final-project/Classification/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_params = {\n",
    "    'model': 'cnn',\n",
    "    'learning_rate': 0.001,\n",
    "    'layers': 2,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 128,\n",
    "    'filters': 64,\n",
    "    'dropout_rate': 0.2,\n",
    "    'kernel_size': 3,\n",
    "    'pool_size': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3244"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3244 samples, validate on 1080 samples\n",
      "Epoch 1/1000\n",
      " - 2s - loss: 0.6812 - acc: 0.5629 - val_loss: 0.6538 - val_acc: 0.6287\n",
      "Epoch 2/1000\n",
      " - 1s - loss: 0.6461 - acc: 0.6255 - val_loss: 0.6257 - val_acc: 0.6444\n",
      "Epoch 3/1000\n",
      " - 1s - loss: 0.6255 - acc: 0.6489 - val_loss: 0.6174 - val_acc: 0.6648\n",
      "Epoch 4/1000\n",
      " - 1s - loss: 0.6108 - acc: 0.6612 - val_loss: 0.6351 - val_acc: 0.6343\n",
      "Epoch 5/1000\n",
      " - 1s - loss: 0.6108 - acc: 0.6732 - val_loss: 0.6061 - val_acc: 0.6685\n",
      "Epoch 6/1000\n",
      " - 1s - loss: 0.5911 - acc: 0.6776 - val_loss: 0.6008 - val_acc: 0.6694\n",
      "Epoch 7/1000\n",
      " - 1s - loss: 0.5770 - acc: 0.6899 - val_loss: 0.5956 - val_acc: 0.6731\n",
      "Epoch 8/1000\n",
      " - 1s - loss: 0.5660 - acc: 0.7013 - val_loss: 0.6095 - val_acc: 0.6546\n",
      "Epoch 9/1000\n",
      " - 1s - loss: 0.5623 - acc: 0.7053 - val_loss: 0.5975 - val_acc: 0.6750\n",
      "Validation accuracy: 0.675000011920929, loss: 0.5975039340831615\n"
     ]
    }
   ],
   "source": [
    "reload(train_model)\n",
    "history, train_pred_probs, val_pred_probs = train_model.train_model(((train_emb, np.array(train_target_emb)), (val_emb, np.array(val_target_emb))), logs_base_dir, **cnn_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
