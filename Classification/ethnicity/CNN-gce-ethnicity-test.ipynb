{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration for code:\n",
    "\n",
    "- https://developers.google.com/machine-learning/guides/text-classification/\n",
    "- https://github.com/google/eng-edu/blob/master/ml/guides/text_classification/load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yulia/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data\n",
    "from helpers import explore_data\n",
    "from helpers import preprocess_data\n",
    "from helpers import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0-rc2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/home/yulia/W266-Final-Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = main_dir+'Classification/data/tokenized/ethnicity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = np.load(os.path.join(token_path, 'train_vec.npy'))\n",
    "val_vec = np.load(os.path.join(token_path, 'val_vec.npy'))\n",
    "test_vec = np.load(os.path.join(token_path, 'test_vec.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = pickle.load(open(os.path.join(token_path, 'word_index.p'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188520, 62840, 62840)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vec), len(val_vec), len(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_path = main_dir+'Classification/data/splits/ethnicity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(outdata_path, 'train_target'), 'rb') as fp:\n",
    "    train_target = pickle.load(fp)\n",
    "with open(os.path.join(outdata_path, 'val_target'), 'rb') as fp:\n",
    "    val_target = pickle.load(fp)\n",
    "with open(os.path.join(outdata_path, 'test_target'), 'rb') as fp:\n",
    "    test_target = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94260, 31420, 31420)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_target), sum(val_target), sum(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model - Full Speech + Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_base_dir = main_dir + 'Classification/logs/ethnicity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_params = {\n",
    "    'model_type': 'cnn',\n",
    "    'word_index': word_index,\n",
    "    'learning_rate': 0.001,\n",
    "    'layers': 2,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 128,\n",
    "    'filters': 64,\n",
    "    'dropout_rate': 0.2,\n",
    "    'embedding_dim': 200,\n",
    "    'kernel_size': 5,\n",
    "    'pool_size': 1,\n",
    "    'max_num_words': 20000,\n",
    "    'use_pretrained_embedding': True,\n",
    "    'is_embedding_trainable': True,\n",
    "    'glove_dir': main_dir + 'data'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "It took 21.8 seconds\n",
      "Preparing embedding matrix.\n",
      "Embedding matrix has been built.\n",
      "Its shape is (20000, 200).\n",
      "It took 0.1 seconds\n",
      "WARNING:tensorflow:From /home/yulia/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/yulia/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 188520 samples, validate on 62840 samples\n",
      "WARNING:tensorflow:From /home/yulia/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/yulia/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1000\n",
      " - 1276s - loss: 0.6008 - acc: 0.6695 - val_loss: 0.5541 - val_acc: 0.7139\n",
      "Epoch 2/1000\n",
      " - 1269s - loss: 0.5469 - acc: 0.7198 - val_loss: 0.5323 - val_acc: 0.7311\n",
      "Epoch 3/1000\n",
      " - 1351s - loss: 0.5196 - acc: 0.7403 - val_loss: 0.5269 - val_acc: 0.7372\n",
      "Epoch 4/1000\n",
      " - 1298s - loss: 0.4968 - acc: 0.7562 - val_loss: 0.5195 - val_acc: 0.7392\n",
      "Epoch 5/1000\n",
      " - 1297s - loss: 0.4762 - acc: 0.7703 - val_loss: 0.5263 - val_acc: 0.7348\n",
      "Epoch 6/1000\n",
      " - 1296s - loss: 0.4543 - acc: 0.7845 - val_loss: 0.5261 - val_acc: 0.7358\n",
      "Validation accuracy: 0.7358370423316956, loss: 0.5261411314553929\n"
     ]
    }
   ],
   "source": [
    "reload(train_model)\n",
    "history, model, train_pred_probs, val_pred_probs = train_model.train_model(((train_vec, train_target), (val_vec, val_target)), logs_base_dir, **cnn_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score/save test and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = main_dir + 'Classification/model/ethnicity/'\n",
    "model.save(model_dir+'cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62840/62840 [==============================] - 144s 2ms/sample - loss: 0.5273 - acc: 0.7366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5273021834849704, 0.73658496]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_probs = model.predict(test_vec)\n",
    "model.evaluate(test_vec, test_target, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_path = main_dir+'Classification/data/probs/ethnicity/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(probs_path+'val_pred_probs_cnn', val_pred_probs, allow_pickle=True, fix_imports=True)\n",
    "np.save(probs_path+'test_pred_probs_cnn', test_pred_probs, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_path = main_dir+'Classification/data/splits/ethnicity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(outdata_path, 'train_list'), 'rb') as fp:\n",
    "    train = pickle.load(fp)\n",
    "with open(os.path.join(outdata_path, 'train_ids'), 'rb') as fp:\n",
    "    train_ids = pickle.load(fp)\n",
    "with open(os.path.join(outdata_path, 'val_list'), 'rb') as fp:\n",
    "    val = pickle.load(fp)\n",
    "with open(os.path.join(outdata_path, 'val_ids'), 'rb') as fp:\n",
    "    val_ids = pickle.load(fp)\n",
    "with open(os.path.join(outdata_path, 'test_list'), 'rb') as fp:\n",
    "    test = pickle.load(fp)\n",
    "with open(os.path.join(outdata_path, 'test_ids'), 'rb') as fp:\n",
    "    test_ids = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data has 188520 speeches\n",
      "It was split into 1507038 chunks\n",
      "Checks on ids and target 1507038 1507038\n",
      "Original target mean 0.5\n",
      "New target mean 0.49381103860685666\n",
      "Original data has 62840 speeches\n",
      "It was split into 503395 chunks\n",
      "Checks on ids and target 503395 503395\n",
      "Original target mean 0.5\n",
      "New target mean 0.4916020222687949\n",
      "Original data has 62840 speeches\n",
      "It was split into 507051 chunks\n",
      "Checks on ids and target 507051 507051\n",
      "Original target mean 0.5\n",
      "New target mean 0.49672715367882125\n",
      "\n",
      "It took 11.2 seconds to create the dictionary\n"
     ]
    }
   ],
   "source": [
    "reload(preprocess_data)\n",
    "start_time = time.time()\n",
    "train_chunk, train_ids_chunk, train_target_chunk = preprocess_data.split_speech_to_chunks(train, train_ids, train_target, max_len=chunk_len)\n",
    "val_chunk, val_ids_chunk, val_target_chunk = preprocess_data.split_speech_to_chunks(val, val_ids, val_target, max_len=chunk_len)\n",
    "test_chunk, test_ids_chunk, test_target_chunk = preprocess_data.split_speech_to_chunks(test, test_ids, test_target, max_len=chunk_len)\n",
    "print(\"\\nIt took {:.1f} seconds to create the dictionary\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(outdata_path, 'val_ids_chunk'), 'wb') as fp:\n",
    "    pickle.dump(val_ids_chunk, fp)\n",
    "with open(os.path.join(outdata_path, 'test_ids_chunk'), 'wb') as fp:\n",
    "    pickle.dump(val_ids_chunk, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = np.load(os.path.join(token_path, 'train_vec_chunk.npy'))\n",
    "val_vec = np.load(os.path.join(token_path, 'val_vec_chunk.npy'))\n",
    "test_vec = np.load(os.path.join(token_path, 'test_vec_chunk.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "It took 21.8 seconds\n",
      "Preparing embedding matrix.\n",
      "Embedding matrix has been built.\n",
      "Its shape is (20000, 200).\n",
      "It took 0.1 seconds\n",
      "Train on 1507038 samples, validate on 503395 samples\n",
      "Epoch 1/1000\n",
      " - 1012s - loss: 0.6161 - acc: 0.6540 - val_loss: 0.6037 - val_acc: 0.6689\n",
      "Epoch 2/1000\n",
      " - 943s - loss: 0.5930 - acc: 0.6776 - val_loss: 0.5986 - val_acc: 0.6731\n",
      "Epoch 3/1000\n",
      " - 1015s - loss: 0.5817 - acc: 0.6881 - val_loss: 0.5983 - val_acc: 0.6749\n",
      "Epoch 4/1000\n",
      " - 1480s - loss: 0.5714 - acc: 0.6971 - val_loss: 0.5990 - val_acc: 0.6734\n",
      "Epoch 5/1000\n",
      " - 1492s - loss: 0.5609 - acc: 0.7058 - val_loss: 0.5968 - val_acc: 0.6746\n",
      "Epoch 6/1000\n",
      " - 1490s - loss: 0.5509 - acc: 0.7135 - val_loss: 0.5984 - val_acc: 0.6748\n",
      "Epoch 7/1000\n",
      " - 1488s - loss: 0.5406 - acc: 0.7216 - val_loss: 0.6016 - val_acc: 0.6741\n",
      "Validation accuracy: 0.674077033996582, loss: 0.6016489580007957\n"
     ]
    }
   ],
   "source": [
    "history_chunk, model, train_pred_probs_chunk, val_pred_probs_chunk = train_model.train_model(((train_vec, train_target_chunk), (val_vec, val_target_chunk)), logs_base_dir, **cnn_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir+'cnn_chunk_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507051/507051 [==============================] - 95s 188us/sample - loss: 0.6002 - acc: 0.6741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6001986647931018, 0.67413926]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_probs_chunk = model.predict(test_vec)\n",
    "model.evaluate(test_vec, test_target_chunk, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(probs_path+'val_pred_probs_cnn_chunk', val_pred_probs_chunk, allow_pickle=True, fix_imports=True)\n",
    "np.save(probs_path+'test_pred_probs_cnn_chunk', test_pred_probs_chunk, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>probs_min</th>\n",
       "      <th>probs_max</th>\n",
       "      <th>probs_mean</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "      <th>preds_probs_mean</th>\n",
       "      <th>preds_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000470</td>\n",
       "      <td>0.191976</td>\n",
       "      <td>0.489970</td>\n",
       "      <td>0.340973</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000000543</td>\n",
       "      <td>0.103388</td>\n",
       "      <td>0.491666</td>\n",
       "      <td>0.261479</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000888</td>\n",
       "      <td>0.330469</td>\n",
       "      <td>0.944494</td>\n",
       "      <td>0.616547</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000000889</td>\n",
       "      <td>0.273495</td>\n",
       "      <td>0.946655</td>\n",
       "      <td>0.754324</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000000904</td>\n",
       "      <td>0.032570</td>\n",
       "      <td>0.751707</td>\n",
       "      <td>0.499733</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ids  probs_min  probs_max  probs_mean  target     preds  \\\n",
       "0  1000000470   0.191976   0.489970    0.340973       1  0.000000   \n",
       "1  1000000543   0.103388   0.491666    0.261479       0  0.000000   \n",
       "2  1000000888   0.330469   0.944494    0.616547       1  0.666667   \n",
       "3  1000000889   0.273495   0.946655    0.754324       1  0.750000   \n",
       "4  1000000904   0.032570   0.751707    0.499733       1  0.600000   \n",
       "\n",
       "   preds_probs_mean  preds_mean  \n",
       "0                 0           0  \n",
       "1                 0           0  \n",
       "2                 1           1  \n",
       "3                 1           1  \n",
       "4                 0           1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds_chunk = val_pred_probs_chunk > 0.5\n",
    "pred_df = pd.DataFrame({'ids': val_ids_chunk, 'target': val_target_chunk, 'probs': val_pred_probs_chunk.flatten(), 'preds': val_preds_chunk.flatten()})\n",
    "f = {'probs': [min, max, 'mean']}\n",
    "pred_aggr_df = pred_df.groupby('ids').agg(f).reset_index()\n",
    "pred_aggr_df.columns = ['ids', 'probs_min', 'probs_max', 'probs_mean']\n",
    "pred_aggr_df = pred_aggr_df.merge(pred_df[['target', 'ids']].groupby('ids').mean().reset_index(), on='ids')\n",
    "pred_aggr_df = pred_aggr_df.merge(pred_df[['preds', 'ids']].groupby('ids').mean().reset_index(), on='ids')\n",
    "pred_aggr_df['preds_probs_mean'] = pred_aggr_df['probs_mean'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "pred_aggr_df['preds_mean'] = pred_aggr_df['preds'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "pred_aggr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_orig</th>\n",
       "      <th>ids</th>\n",
       "      <th>probs_min</th>\n",
       "      <th>probs_max</th>\n",
       "      <th>probs_mean</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "      <th>preds_probs_mean</th>\n",
       "      <th>preds_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1010243717</td>\n",
       "      <td>0.276729</td>\n",
       "      <td>0.518066</td>\n",
       "      <td>0.403256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1120097305</td>\n",
       "      <td>0.173403</td>\n",
       "      <td>0.886079</td>\n",
       "      <td>0.566222</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1020183502</td>\n",
       "      <td>0.425271</td>\n",
       "      <td>0.425271</td>\n",
       "      <td>0.425271</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1080187463</td>\n",
       "      <td>0.148529</td>\n",
       "      <td>0.803117</td>\n",
       "      <td>0.421413</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1090010764</td>\n",
       "      <td>0.166191</td>\n",
       "      <td>0.900182</td>\n",
       "      <td>0.547798</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_orig         ids  probs_min  probs_max  probs_mean  target  \\\n",
       "0            1  1010243717   0.276729   0.518066    0.403256       1   \n",
       "1            1  1120097305   0.173403   0.886079    0.566222       1   \n",
       "2            1  1020183502   0.425271   0.425271    0.425271       1   \n",
       "3            1  1080187463   0.148529   0.803117    0.421413       1   \n",
       "4            1  1090010764   0.166191   0.900182    0.547798       1   \n",
       "\n",
       "      preds  preds_probs_mean  preds_mean  \n",
       "0  0.250000                 0           0  \n",
       "1  0.500000                 1           0  \n",
       "2  0.000000                 0           0  \n",
       "3  0.444444                 0           0  \n",
       "4  0.500000                 1           0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df = pd.DataFrame({'target_orig': val_target, 'ids': val_ids})\n",
    "original_df = original_df.merge(pred_aggr_df, on=\"ids\")\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_aggr_chunk = original_df.preds_probs_mean\n",
    "val_pred_probs_aggr_chunk = original_df.probs_mean\n",
    "# val_aggr_target = pred_aggr_df.target\n",
    "# val_aggr_ids = pred_aggr_df.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(probs_path+'val_pred_probs_cnn_aggr_chunk', val_pred_probs_aggr_chunk, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74484404837683"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(val_target==val_pred_aggr_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>probs_min</th>\n",
       "      <th>probs_max</th>\n",
       "      <th>probs_mean</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "      <th>preds_probs_mean</th>\n",
       "      <th>preds_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000141</td>\n",
       "      <td>0.315687</td>\n",
       "      <td>0.798025</td>\n",
       "      <td>0.583022</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000000591</td>\n",
       "      <td>0.078930</td>\n",
       "      <td>0.530047</td>\n",
       "      <td>0.248486</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000699</td>\n",
       "      <td>0.205669</td>\n",
       "      <td>0.292894</td>\n",
       "      <td>0.249282</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000000747</td>\n",
       "      <td>0.291679</td>\n",
       "      <td>0.433615</td>\n",
       "      <td>0.362647</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000000750</td>\n",
       "      <td>0.326837</td>\n",
       "      <td>0.986345</td>\n",
       "      <td>0.656591</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ids  probs_min  probs_max  probs_mean  target     preds  \\\n",
       "0  1000000141   0.315687   0.798025    0.583022       1  0.736842   \n",
       "1  1000000591   0.078930   0.530047    0.248486       0  0.066667   \n",
       "2  1000000699   0.205669   0.292894    0.249282       0  0.000000   \n",
       "3  1000000747   0.291679   0.433615    0.362647       0  0.000000   \n",
       "4  1000000750   0.326837   0.986345    0.656591       0  0.500000   \n",
       "\n",
       "   preds_probs_mean  preds_mean  \n",
       "0                 1           1  \n",
       "1                 0           0  \n",
       "2                 0           0  \n",
       "3                 0           0  \n",
       "4                 1           0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_chunk = test_pred_probs_chunk > 0.5\n",
    "pred_df = pd.DataFrame({'ids': test_ids_chunk, 'target': test_target_chunk, 'probs': test_pred_probs_chunk.flatten(), 'preds': test_preds_chunk.flatten()})\n",
    "f = {'probs': [min, max, 'mean']}\n",
    "pred_aggr_df = pred_df.groupby('ids').agg(f).reset_index()\n",
    "pred_aggr_df.columns = ['ids', 'probs_min', 'probs_max', 'probs_mean']\n",
    "pred_aggr_df = pred_aggr_df.merge(pred_df[['target', 'ids']].groupby('ids').mean().reset_index(), on='ids')\n",
    "pred_aggr_df = pred_aggr_df.merge(pred_df[['preds', 'ids']].groupby('ids').mean().reset_index(), on='ids')\n",
    "pred_aggr_df['preds_probs_mean'] = pred_aggr_df['probs_mean'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "pred_aggr_df['preds_mean'] = pred_aggr_df['preds'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "pred_aggr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_orig</th>\n",
       "      <th>ids</th>\n",
       "      <th>probs_min</th>\n",
       "      <th>probs_max</th>\n",
       "      <th>probs_mean</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "      <th>preds_probs_mean</th>\n",
       "      <th>preds_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>980211229</td>\n",
       "      <td>0.511725</td>\n",
       "      <td>0.725628</td>\n",
       "      <td>0.652895</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1120080577</td>\n",
       "      <td>0.274669</td>\n",
       "      <td>0.741413</td>\n",
       "      <td>0.546952</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>970087805</td>\n",
       "      <td>0.175938</td>\n",
       "      <td>0.937863</td>\n",
       "      <td>0.622395</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1070071796</td>\n",
       "      <td>0.620812</td>\n",
       "      <td>0.620812</td>\n",
       "      <td>0.620812</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1130075886</td>\n",
       "      <td>0.367619</td>\n",
       "      <td>0.922119</td>\n",
       "      <td>0.639309</td>\n",
       "      <td>1</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target_orig         ids  probs_min  probs_max  probs_mean  target  \\\n",
       "0            1   980211229   0.511725   0.725628    0.652895       1   \n",
       "1            1  1120080577   0.274669   0.741413    0.546952       1   \n",
       "2            1   970087805   0.175938   0.937863    0.622395       1   \n",
       "3            1  1070071796   0.620812   0.620812    0.620812       1   \n",
       "4            1  1130075886   0.367619   0.922119    0.639309       1   \n",
       "\n",
       "      preds  preds_probs_mean  preds_mean  \n",
       "0  1.000000                 1           1  \n",
       "1  0.714286                 1           1  \n",
       "2  0.666667                 1           1  \n",
       "3  1.000000                 1           1  \n",
       "4  0.647059                 1           1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df = pd.DataFrame({'target_orig': test_target, 'ids': test_ids})\n",
    "original_df = original_df.merge(pred_aggr_df, on=\"ids\")\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_aggr_chunk = original_df.preds_probs_mean\n",
    "test_pred_probs_aggr_chunk = original_df.probs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(probs_path+'test_pred_probs_cnn_aggr_chunk', test_pred_probs_aggr_chunk, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
