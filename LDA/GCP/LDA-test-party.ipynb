{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources: \n",
    "\n",
    "- Tutorial: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "- Another tutorial: https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html\n",
    "    - This tutorial has a few useful links inside\n",
    "- LDA model reference: https://radimrehurek.com/gensim/models/ldamulticore.html\n",
    "- LDA coherence model reference: https://radimrehurek.com/gensim/models/coherencemodel.html\n",
    "- Coherence metrics research paper: http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from importlib import reload\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gensim if necessary\n",
    "#!pip install --upgrade gensim \n",
    "#!pip install google-compute-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbm221/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data\n",
    "from helpers import explore_data\n",
    "from helpers import preprocess_data\n",
    "from helpers import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/fbm221/W266FinalProject/Code'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/home/fbm221/W266FinalProject'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "indata_path = main_dir + '/Data/Party/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_path = main_dir + '/saved_files/LDA/Party/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(indata_path, 'val_list'), 'rb') as fp:\n",
    "    main_data = pickle.load(fp)\n",
    "with open(os.path.join(indata_path, 'val_ids'), 'rb') as fp:\n",
    "    main_ids = pickle.load(fp)\n",
    "with open(os.path.join(indata_path, 'val_target'), 'rb') as fp:\n",
    "    main_target = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101153"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['mr', 'senator', 'united', 'states', 'president', 'would', 'speaker', 'senate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_speech(speech):\n",
    "    processed_speech = []\n",
    "    for word in simple_preprocess(speech) :\n",
    "        if word not in stop_words:\n",
    "            processed_speech.append(WordNetLemmatizer().lemmatize(word, pos='v'))\n",
    "\n",
    "    return processed_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_speech(speeches):\n",
    "    \n",
    "    speeches_processed = [lemmatize_speech(speech) for speech in speeches]\n",
    "    \n",
    "    bigram = Phrases(sentences=speeches_processed, \n",
    "                     scoring='npmi',\n",
    "                     min_count=30, \n",
    "                     threshold=0.5)\n",
    "    \n",
    "    trigram = Phrases(sentences=bigram[speeches_processed], \n",
    "                      scoring='npmi',\n",
    "                      min_count=30, \n",
    "                      threshold=0.5)  \n",
    "    \n",
    "    bigram_mod = Phraser(bigram)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "    \n",
    "    return [trigram_mod[bigram_mod[speech]] for speech in speeches_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It took 2817.3 seconds to process the data\n"
     ]
    }
   ],
   "source": [
    "# Only run this the first time. Otherwise import data_preprocessed below\n",
    "start_time = time.time()\n",
    "data_preprocessed = preprocess_speech(main_data)\n",
    "print(\"\\nIt took {:.1f} seconds to process the data\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this the first time. Otherwise import data_preprocessed below\n",
    "# Save data_preprocessed\n",
    "\n",
    "pickle.dump(data_preprocessed, open(os.path.join(indata_path, 'data_preprocessed'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_preprocessed\n",
    "with open(os.path.join(indata_path, 'data_preprocessed'), 'rb') as fp:\n",
    "    data_preprocessed = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens removed: 244500. 30538 tokens will be used\n"
     ]
    }
   ],
   "source": [
    "# Only run this the first time. Otherwise import word_index below\n",
    "word_index = Dictionary(data_preprocessed)\n",
    "word_index_check = Dictionary(data_preprocessed)\n",
    "word_index.filter_extremes(no_below=10, no_above=0.3)\n",
    "print(\"Number of tokens removed: {}. {} tokens will be used\".format(len(word_index_check) - len(word_index), len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this the first time. Otherwise import word_index below\n",
    "word_index.save_as_text(fname=os.path.join(indata_path, 'word_index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import word_index\n",
    "word_index = Dictionary.load_from_text(fname=os.path.join(indata_path, 'word_index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words:\n",
      "support 30191\n",
      "also 30038\n",
      "people 29525\n",
      "many 28897\n",
      "years 28771\n",
      "know 28037\n",
      "today 28016\n",
      "need 27764\n",
      "us 27277\n",
      "want 27001\n",
      "think 25832\n",
      "like 25637\n",
      "come 25597\n",
      "congress 25234\n",
      "provide 25135\n",
      "committee 24651\n",
      "state 24478\n",
      "get 23534\n",
      "first 23272\n",
      "act 23144\n"
     ]
    }
   ],
   "source": [
    "most_frequent = sorted(word_index.dfs.items(), key=lambda x: -x[1])[:20]\n",
    "print(\"Most frequent words:\")\n",
    "for i in most_frequent:\n",
    "    print(word_index[i[0]], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [word_index.doc2bow(speech) for speech in data_preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual speech check:\n",
      "Word 419 (\"follow\") appears 2 time(s).\n",
      "Word 753 (\"military\") appears 2 time(s).\n",
      "Word 1146 (\"fiscal_year\") appears 2 time(s).\n",
      "Word 1834 (\"activities\") appears 2 time(s).\n",
      "Word 1 (\"appropriations\") appears 1 time(s).\n",
      "Word 108 (\"add\") appears 1 time(s).\n",
      "Word 147 (\"defense\") appears 1 time(s).\n",
      "Word 193 (\"order\") appears 1 time(s).\n",
      "Word 400 (\"authorize\") appears 1 time(s).\n",
      "Word 501 (\"propose\") appears 1 time(s).\n"
     ]
    }
   ],
   "source": [
    "print(\"Individual speech check:\")\n",
    "check = sorted(bow_corpus[100], key=lambda x: -x[1])\n",
    "for i in range(min(len(check),10)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time(s).\".format(check[i][0], \n",
    "                                                     word_index[check[i][0]], \n",
    "                                                     check[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, speeches, \n",
    "                            topics_range = range(1,10,1),\n",
    "                            chunksize=100,\n",
    "                            passes=10,\n",
    "                            iterations=10,\n",
    "                            eval_every=None,\n",
    "                            workers=8,\n",
    "                            random_state=100,\n",
    "                            per_word_topics=False,\n",
    "                            coherence='c_v'):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    model_list, coherence_values = [], []\n",
    "\n",
    "    for num_topics in topics_range:\n",
    "        model = LdaMulticore(corpus=corpus,\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=num_topics, \n",
    "                             chunksize=chunksize,\n",
    "                             passes=passes,\n",
    "                             iterations=iterations,\n",
    "                             eval_every=eval_every,\n",
    "                             workers=workers,\n",
    "                             random_state=random_state,\n",
    "                             per_word_topics=per_word_topics)\n",
    "        \n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, \n",
    "                                         texts=speeches, \n",
    "                                         dictionary=dictionary, \n",
    "                                         coherence=coherence)\n",
    "        \n",
    "        cv = coherence_model.get_coherence()\n",
    "        coherence_values.append(cv)\n",
    "        print(\"Coherence for {} topics is {:.2f}\".format(num_topics, cv))\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Params 1\n",
      "Coherence for 1 topics is 0.28\n",
      "Coherence for 6 topics is 0.42\n",
      "Coherence for 11 topics is 0.49\n",
      "Coherence for 16 topics is 0.49\n",
      "Coherence for 21 topics is 0.49\n",
      "Coherence for 26 topics is 0.53\n",
      "Coherence for 31 topics is 0.51\n",
      "Coherence for 36 topics is 0.54\n",
      "CPU times: user 1h 30min 17s, sys: 7min, total: 1h 37min 18s\n",
      "Wall time: 3h 14min 17s\n",
      "LDA Params 2\n",
      "Coherence for 1 topics is 0.28\n",
      "Coherence for 6 topics is 0.44\n",
      "Coherence for 11 topics is 0.48\n",
      "Coherence for 16 topics is 0.50\n",
      "Coherence for 21 topics is 0.51\n",
      "Coherence for 26 topics is 0.54\n",
      "Coherence for 31 topics is 0.53\n",
      "Coherence for 36 topics is 0.54\n",
      "CPU times: user 3h 31min 16s, sys: 13min 32s, total: 3h 44min 48s\n",
      "Wall time: 2h 14min 9s\n",
      "LDA Params 3\n",
      "Coherence for 1 topics is 0.28\n",
      "Coherence for 6 topics is 0.43\n",
      "Coherence for 11 topics is 0.48\n",
      "Coherence for 16 topics is 0.53\n",
      "Coherence for 21 topics is 0.50\n",
      "Coherence for 26 topics is 0.52\n",
      "Coherence for 31 topics is 0.53\n",
      "Coherence for 36 topics is 0.54\n",
      "CPU times: user 5h 15min 20s, sys: 20min 5s, total: 5h 35min 26s\n",
      "Wall time: 3h 9min 17s\n",
      "LDA Params 4\n",
      "Coherence for 1 topics is 0.28\n",
      "Coherence for 6 topics is 0.44\n",
      "Coherence for 11 topics is 0.48\n",
      "Coherence for 16 topics is 0.51\n",
      "Coherence for 21 topics is 0.50\n",
      "Coherence for 26 topics is 0.52\n",
      "Coherence for 31 topics is 0.53\n",
      "Coherence for 36 topics is 0.55\n",
      "CPU times: user 7h 19s, sys: 27min 15s, total: 7h 27min 34s\n",
      "Wall time: 4h 8min 37s\n"
     ]
    }
   ],
   "source": [
    "# # pre-process speech paramters:\n",
    "#     bigram = Phrases(sentences=speeches_processed, \n",
    "#                      scoring='npmi',\n",
    "#                      min_count=30, \n",
    "#                      threshold=0.5)\n",
    "    \n",
    "#     trigram = Phrases(sentences=bigram[speeches_processed], \n",
    "#                       scoring='npmi',\n",
    "#                       min_count=30, \n",
    "#                       threshold=0.5) \n",
    "\n",
    "### Running LDA on lda_params1 ###\n",
    "print(\"LDA Params 1\")\n",
    "lda_params1 = {\n",
    "    'topics_range': range(1,41,5),\n",
    "    'chunksize': 100,\n",
    "    'passes': 5,\n",
    "    'iterations': 5,\n",
    "    'eval_every': None,\n",
    "    'workers': 8,\n",
    "    'random_state': 100,\n",
    "    'per_word_topics': False,\n",
    "    'coherence': 'c_v'\n",
    "}\n",
    "%time model_list1, coherence_values1 = compute_coherence_values(dictionary=word_index, corpus=bow_corpus, speeches=data_preprocessed, **lda_params1)\n",
    "\n",
    "\n",
    "### Running LDA on lda_params2 ###\n",
    "print(\"LDA Params 2\")\n",
    "lda_params2 = {\n",
    "    'topics_range': range(1,41,5),\n",
    "    'chunksize': 100,\n",
    "    'passes': 10,\n",
    "    'iterations': 10,\n",
    "    'eval_every': None,\n",
    "    'workers': 8,\n",
    "    'random_state': 100,\n",
    "    'per_word_topics': False,\n",
    "    'coherence': 'c_v'\n",
    "}\n",
    "%time model_list2, coherence_values2 = compute_coherence_values(dictionary=word_index, corpus=bow_corpus, speeches=data_preprocessed, **lda_params2)\n",
    "\n",
    "### Running LDA on lda_params3 ###\n",
    "print(\"LDA Params 3\")\n",
    "lda_params3 = {\n",
    "    'topics_range': range(1,41,5),\n",
    "    'chunksize': 100,\n",
    "    'passes': 15,\n",
    "    'iterations': 15,\n",
    "    'eval_every': None,\n",
    "    'workers': 8,\n",
    "    'random_state': 100,\n",
    "    'per_word_topics': False,\n",
    "    'coherence': 'c_v'\n",
    "}\n",
    "%time model_list3, coherence_values3 = compute_coherence_values(dictionary=word_index, corpus=bow_corpus, speeches=data_preprocessed, **lda_params3)\n",
    "\n",
    "### Running LDA on lda_params4 ###\n",
    "print(\"LDA Params 4\")\n",
    "lda_params4 = {\n",
    "    'topics_range': range(1,41,5),\n",
    "    'chunksize': 100,\n",
    "    'passes': 20,\n",
    "    'iterations': 20,\n",
    "    'eval_every': None,\n",
    "    'workers': 8,\n",
    "    'random_state': 100,\n",
    "    'per_word_topics': False,\n",
    "    'coherence': 'c_v'\n",
    "}\n",
    "%time model_list4, coherence_values4 = compute_coherence_values(dictionary=word_index, corpus=bow_corpus, speeches=data_preprocessed, **lda_params4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_range = [1, 6, 11, 16, 21, 26, 31, 36]\n",
    "\n",
    "for model_list in [model_list1, model_list2, model_list3, model_list4]:\n",
    "    for model in model_list:\n",
    "        for num_topics in topics_range:\n",
    "            if model_list == model_list1:\n",
    "                model.save(outdata_path + 'lda_model_list1_' + str(num_topics))\n",
    "            if model_list == model_list2:\n",
    "                model.save(outdata_path + 'lda_model_list2_' + str(num_topics))\n",
    "            if model_list == model_list3:\n",
    "                model.save(outdata_path + 'lda_model_list3_' + str(num_topics))\n",
    "            if model_list == model_list4:\n",
    "                model.save(outdata_path + 'lda_model_list4_' + str(num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the model based on the highest coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of topics with the highest coherence value in each model\n",
    "\n",
    "# Model 1\n",
    "num_topics1 = list(lda_params1['topics_range'])[np.argmax(coherence_values1)]\n",
    "optimal_model1 = model_list1[np.argmax(coherence_values1)]\n",
    "cv1 = [np.argmax(coherence_values1)]\n",
    "passes1 = optimal_model1['passes']\n",
    "iterations1 = optimal_model1['iterations']\n",
    "\n",
    "# Model 2\n",
    "num_topics2 = list(lda_params2['topics_range'])[np.argmax(coherence_values2)]\n",
    "optimal_model2 = model_list2[np.argmax(coherence_values2)]\n",
    "cv2 = [np.argmax(coherence_values2)]\n",
    "passes2 = optimal_model2['passes']\n",
    "iterations2 = optimal_model2['iterations']\n",
    "\n",
    "# Model 3\n",
    "num_topics3 = list(lda_params3['topics_range'])[np.argmax(coherence_values3)]\n",
    "optimal_model3 = model_list3[np.argmax(coherence_values3)]\n",
    "cv3 = [np.argmax(coherence_values3)]\n",
    "passes3 = optimal_model3['passes']\n",
    "iterations3 = optimal_model3['iterations']\n",
    "\n",
    "# Model 4\n",
    "num_topics4 = list(lda_params4['topics_range'])[np.argmax(coherence_values4)]\n",
    "optimal_model4 = model_list4[np.argmax(coherence_values4)]\n",
    "cv4 = [np.argmax(coherence_values4)]\n",
    "passes4 = optimal_model4['passes']\n",
    "iterations4 = optimal_model4['iterations']\n",
    "\n",
    "# Lists of data\n",
    "num_topics = [num_topics1, num_topics2, num_topics3, num_topics4]\n",
    "optimal_models = [optimal_model1, optimal_model2, optimal_model3, optimal_model4]\n",
    "cvs = [cv1, cv2, cv3, cv4]\n",
    "passes = [passes1, passes2, passes3, passes4]\n",
    "iterations = [iterations1, iterations2, iterations3, iterations4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: 0.5512.\n",
      "[([(0.043567196, 'defense'),\n",
      "   (0.025677465, 'military'),\n",
      "   (0.024918953, 'arm'),\n",
      "   (0.014024991, 'nuclear'),\n",
      "   (0.012702031, 'force'),\n",
      "   (0.010622187, 'security'),\n",
      "   (0.008375348, 'aircraft'),\n",
      "   (0.0071074055, 'weapons'),\n",
      "   (0.0070584887, 'strategic'),\n",
      "   (0.006676709, 'must')],\n",
      "  0.7334587837592823),\n",
      " ([(0.035327453, 'budget'),\n",
      "   (0.028354025, 'tax'),\n",
      "   (0.019736348, 'billion'),\n",
      "   (0.01870228, 'cut'),\n",
      "   (0.01565298, 'percent'),\n",
      "   (0.014587059, 'increase'),\n",
      "   (0.014322598, 'spend'),\n",
      "   (0.012709231, 'year'),\n",
      "   (0.012372468, 'program'),\n",
      "   (0.010558332, 'federal')],\n",
      "  0.6686538163011398),\n",
      " ([(0.047873456, 'benefit'),\n",
      "   (0.032219175, 'social_security'),\n",
      "   (0.030723514, 'veterans'),\n",
      "   (0.025986213, 'service'),\n",
      "   (0.025632817, 'pay'),\n",
      "   (0.017189408, 'elderly'),\n",
      "   (0.016738096, 'employees'),\n",
      "   (0.01629803, 'retire'),\n",
      "   (0.016189791, 'receive'),\n",
      "   (0.012604101, 'health')],\n",
      "  0.6436194626358426),\n",
      " ([(0.027453482, 'go'),\n",
      "   (0.026455317, 'say'),\n",
      "   (0.024201903, 'think'),\n",
      "   (0.015516921, 'amendment'),\n",
      "   (0.013690555, 'get'),\n",
      "   (0.012984482, 'want'),\n",
      "   (0.012516508, 'gentleman'),\n",
      "   (0.012014483, 'know'),\n",
      "   (0.0118320035, 'us'),\n",
      "   (0.010728142, 'come')],\n",
      "  0.5826587338306874),\n",
      " ([(0.026360726, 'committee'),\n",
      "   (0.021858891, 'bill'),\n",
      "   (0.015500581, 'amendment'),\n",
      "   (0.011499001, 'house'),\n",
      "   (0.010877031, 'act'),\n",
      "   (0.010164288, 'provision'),\n",
      "   (0.008855221, 'congress'),\n",
      "   (0.007205947, 'legislation'),\n",
      "   (0.006929655, 'vote'),\n",
      "   (0.006767634, 'report')],\n",
      "  0.5649765200918802),\n",
      " ([(0.018135251, 'price'),\n",
      "   (0.017252527, 'market'),\n",
      "   (0.012636558, 'company'),\n",
      "   (0.012511874, 'percent'),\n",
      "   (0.011479954, 'industry'),\n",
      "   (0.011251162, 'interest'),\n",
      "   (0.010812093, 'american'),\n",
      "   (0.0104544535, 'cost'),\n",
      "   (0.010049434, 'farm'),\n",
      "   (0.01003932, 'trade')],\n",
      "  0.5486497349307037),\n",
      " ([(0.03461909, 'women'),\n",
      "   (0.027559396, 'right'),\n",
      "   (0.027434064, 'law'),\n",
      "   (0.025018526, 'court'),\n",
      "   (0.019354567, 'legal'),\n",
      "   (0.013376914, 'state'),\n",
      "   (0.011520214, 'case'),\n",
      "   (0.0114101, 'constitution'),\n",
      "   (0.010023293, 'congress'),\n",
      "   (0.009886721, 'may')],\n",
      "  0.5413810107725021),\n",
      " ([(0.022360262, 'energy'),\n",
      "   (0.022338914, 'project'),\n",
      "   (0.021773845, 'cost'),\n",
      "   (0.017111484, 'million'),\n",
      "   (0.01683628, 'construction'),\n",
      "   (0.011432972, 'job'),\n",
      "   (0.010262368, 'need'),\n",
      "   (0.009989022, 'study'),\n",
      "   (0.009813226, 'percent'),\n",
      "   (0.009351139, 'build')],\n",
      "  0.5028916597224156),\n",
      " ([(0.028752062, 'program'),\n",
      "   (0.0128904395, 'provide'),\n",
      "   (0.011871207, 'federal'),\n",
      "   (0.01035491, 'support'),\n",
      "   (0.010239238, 'bill'),\n",
      "   (0.0095388, 'need'),\n",
      "   (0.009364193, 'fund'),\n",
      "   (0.009352877, 'legislation'),\n",
      "   (0.0076676537, 'act'),\n",
      "   (0.007535688, 'work')],\n",
      "  0.49142351160370457),\n",
      " ([(0.009839254, 'international'),\n",
      "   (0.008527134, 'government'),\n",
      "   (0.00783002, 'world'),\n",
      "   (0.0073748855, 'nations'),\n",
      "   (0.0073079853, 'resolution'),\n",
      "   (0.0073002786, 'american'),\n",
      "   (0.0066294326, 'policy'),\n",
      "   (0.006582938, 'support'),\n",
      "   (0.0057060607, 'must'),\n",
      "   (0.0056380066, 'concern')],\n",
      "  0.46197824881253824),\n",
      " ([(0.013242604, 'school'),\n",
      "   (0.0121687185, 'serve'),\n",
      "   (0.010848959, 'service'),\n",
      "   (0.010504803, 'national'),\n",
      "   (0.008745615, 'community'),\n",
      "   (0.008304894, 'public'),\n",
      "   (0.007923313, 'education'),\n",
      "   (0.0076833246, 'state'),\n",
      "   (0.0070715295, 'article'),\n",
      "   (0.006759144, 'years')],\n",
      "  0.4548613522344406),\n",
      " ([(0.009868913, 'years'),\n",
      "   (0.008662982, 'us'),\n",
      "   (0.007119801, 'great'),\n",
      "   (0.0070509473, 'live'),\n",
      "   (0.0068470724, 'work'),\n",
      "   (0.0068441564, 'many'),\n",
      "   (0.006537477, 'day'),\n",
      "   (0.0063592773, 'know'),\n",
      "   (0.0062496183, 'world'),\n",
      "   (0.006236514, 'today')],\n",
      "  0.4197258520479156)]\n"
     ]
    }
   ],
   "source": [
    "# number of words per topic to display (can be any number within vocabulary size)\n",
    "num_words = 10\n",
    "\n",
    "# Model 1\n",
    "top_topics1 = optimal_model1.top_topics(corpus=bow_corpus, \n",
    "                                      texts=data_preprocessed,\n",
    "                                      coherence=lda_params1['coherence'], \n",
    "                                      topn=num_words)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence1 = sum([t[1] for t in top_topics1]) / num_topics1\n",
    "\n",
    "# Model 2\n",
    "top_topics2 = optimal_model2.top_topics(corpus=bow_corpus, \n",
    "                                      texts=data_preprocessed,\n",
    "                                      coherence=lda_params2['coherence'], \n",
    "                                      topn=num_words)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence2 = sum([t[1] for t in top_topics2]) / num_topics2\n",
    "\n",
    "# Model 3\n",
    "top_topics3 = optimal_model3.top_topics(corpus=bow_corpus, \n",
    "                                      texts=data_preprocessed,\n",
    "                                      coherence=lda_params3['coherence'], \n",
    "                                      topn=num_words)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence3 = sum([t[1] for t in top_topics3]) / num_topics3\n",
    "\n",
    "# Model 4\n",
    "top_topics4 = optimal_model4.top_topics(corpus=bow_corpus, \n",
    "                                      texts=data_preprocessed,\n",
    "                                      coherence=lda_params4['coherence'], \n",
    "                                      topn=num_words)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence4 = sum([t[1] for t in top_topics4]) / num_topics4\n",
    "\n",
    "# Lists of data\n",
    "models = [model1, model2, model3, model4]\n",
    "top_topics = [top_topics1, top_topics2, top_topics3, top_topics4]\n",
    "avg_topic_coherence = [avg_topic_coherence1, avg_topic_coherence2, avg_topic_coherence3, avg_topic_coherence4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparisons = pd.DataFrame(\n",
    "    {'Model Number': models,\n",
    "     'Number of Topics': num_topics,\n",
    "     'Coherence Values': cvs,\n",
    "     'Average Topic Coherence': avg_topic_coherence,\n",
    "     'Top Topics': top_topics\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_keywords_dict(ldamodel, num_topics):\n",
    "    topic_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        topic_dict[i] = [word[0] for word in ldamodel.show_topic(i)]\n",
    "    return topic_dict\n",
    "keywords_dict = get_topics_keywords_dict(optimal_model, num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append primary and secondary topics to the speech file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3244, 7)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def append_topic(ldamodel, corpus, speeches, ids, kw_dict):\n",
    "    # Init output\n",
    "    speech_topics_df = pd.DataFrame()\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        sorted_topics = sorted(row, key=lambda x: -x[1])\n",
    "        topic_count = len(sorted_topics)\n",
    "        \n",
    "        topic1_num, topic1_contrib = sorted_topics[0]\n",
    "        topic1_keywords = ','.join(kw_dict[topic1_num])\n",
    "        \n",
    "        if topic_count > 1:\n",
    "            topic2_num, topic2_contrib = sorted_topics[1]\n",
    "        else:\n",
    "            topic2_num, topic2_contrib = -1, 0\n",
    "        \n",
    "        new_row = [topic_count, int(topic1_num), round(topic1_contrib,2), topic1_keywords, int(topic2_num), round(topic2_contrib,2)]\n",
    "        speech_topics_df = speech_topics_df.append(pd.Series(new_row), ignore_index=True)\n",
    "\n",
    "    speech_topics_df = pd.concat([speech_topics_df, pd.Series(ids)], axis=1)\n",
    "    speech_topics_df.columns = ['Topic_Count', 'Prim_Topic', 'Prim_Topic_Contrib', 'Prim_Topic_Keywords', 'Sec_Topic', 'Sec_Topic_Contrib', 'Speech_id']\n",
    "    \n",
    "    return speech_topics_df\n",
    "\n",
    "\n",
    "all_speeches_topics_df = append_topic(ldamodel=optimal_model, \n",
    "                                      corpus=bow_corpus, \n",
    "                                      speeches=data_preprocessed, \n",
    "                                      ids=main_ids,\n",
    "                                      kw_dict=keywords_dict)\n",
    "all_speeches_topics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Count</th>\n",
       "      <th>Prim_Topic</th>\n",
       "      <th>Prim_Topic_Contrib</th>\n",
       "      <th>Prim_Topic_Keywords</th>\n",
       "      <th>Sec_Topic</th>\n",
       "      <th>Sec_Topic_Contrib</th>\n",
       "      <th>Speech_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>go,say,think,amendment,get,want,gentleman,know...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>970228387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>years,us,great,live,work,many,day,know,world,t...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>970020112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>go,say,think,amendment,get,want,gentleman,know...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>970222405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>go,say,think,amendment,get,want,gentleman,know...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>970192743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>program,provide,federal,support,bill,need,fund...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>970005171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Count  Prim_Topic  Prim_Topic_Contrib  \\\n",
       "0          3.0         2.0                0.84   \n",
       "1          6.0        11.0                0.51   \n",
       "2          2.0         2.0                0.65   \n",
       "3          3.0         2.0                0.74   \n",
       "4          5.0        10.0                0.34   \n",
       "\n",
       "                                 Prim_Topic_Keywords  Sec_Topic  \\\n",
       "0  go,say,think,amendment,get,want,gentleman,know...        9.0   \n",
       "1  years,us,great,live,work,many,day,know,world,t...        7.0   \n",
       "2  go,say,think,amendment,get,want,gentleman,know...        4.0   \n",
       "3  go,say,think,amendment,get,want,gentleman,know...        6.0   \n",
       "4  program,provide,federal,support,bill,need,fund...        8.0   \n",
       "\n",
       "   Sec_Topic_Contrib  Speech_id  \n",
       "0               0.13  970228387  \n",
       "1               0.30  970020112  \n",
       "2               0.32  970222405  \n",
       "3               0.18  970192743  \n",
       "4               0.32  970005171  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_speeches_topics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick one speech per document with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting n top speeches per topic\n",
    "top_speeches = 1\n",
    "top_speeches_df = pd.DataFrame()\n",
    "\n",
    "all_speeches_topics_df_grpd = all_speeches_topics_df.groupby('Prim_Topic')\n",
    "\n",
    "for i, grp in all_speeches_topics_df_grpd:\n",
    "    top_speeches_df = pd.concat([top_speeches_df, grp.sort_values(by='Prim_Topic_Contrib', ascending=False).head(top_speeches)], axis=0)\n",
    "\n",
    "top_speeches_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number:  0.0\n",
      "Topic contribution: 0.80\n",
      "Keywords: \n",
      " price,market,company,percent,industry,interest,american,cost,farm,trade\n",
      "Speech: \n",
      " Mr. Chairman. I C rise in opposition to the amendment. 0 Mr. Chairman. I support H.R. 5133. t the Fair Practices Automotive Products Act of 1982. Simply stated. what this bill is about Is helping the U.S. auto industry. For the past 4 years. the industry has been sinking deeper and deeper into depression. Total sales are less than half their 1978 level and almost 1 million workers have lost their jobs. including 280.000 auto workers and another 670.000 workers in auto supply industries. At the same time. sales of imported cars have been rising. Imports from Japan have increased by over 37 percent. and more than 1 of every 5 cars sold in the United States is Japanese built. Overall. imports make up 27 percent of the U.S. car market. We cant continue to allow imports to take a larger share of our market. with the result of lost American Jobs and an- everworsening balance of trade deficit. Without strong. prompt measures to revive the domestic auto industry. overall economic recovery will remain beyond our reach. Traditionally we have always relied on a few basic industries. including autos and housing. to lead the economy to recovery. The importance of the auto industry is easily demonstrated. Even 1n 1981. after 3 years of declining sales and layoffs. there were still almost 2.5 million people employed directly or indirectly In the industry. The additional economic activity generated by a healthy auto industry is just what is needed to pull the country out of this recession. The bill before us will help create and maintain jobs in the auto Industry. It will stop the exporting of Jobs to Japan and will reduce outsourcing by domestic auto companies that ansfera jobs from U.S. workers to ma wwage Third World countries. r The bill takes a simple and direct ap- J roach. It simply requires that auto B rmpanies with more than 100.000 r ales annually here build a certain per- 1 entage of those cars and trucks here. c doesnt establish quotas. or increase t ariffs. It doesnt say. as other coun- n ries have said. \"you cant sell here.\" a All it says is. if youre going to sell t ere. you have to build here. It says if t a auto company is going to control a p zable share of the U.S. car market. n hat company will be required to a )cate production facilities here. andire workers here. and buy auto parts n ere. Its a fair proposal. and the r ffect will be to create or p:eserve 1 f illion jobs that would otherwise be d ost. t The United States has always led t he world as a proponent of free trade. i n an ideal world. free and open trade i etween all countries would be the ule. But we do not live in an ideal world. As we have learned. you cant have ree trade without fair trade. For years. we have refused. in the interests of promoting free trade. to erect pro. ections for U.S. workers. Now we now we cant afford to be so ideallsic. Other autoproducing nations. noably Japan and our major allies in Europe. have trade barriers on automobile imports which are much more stiff than those contained in this bill. We need to pass this bill to provide our own workers with some basic degree of Job security. I strongly urge my colleagues. on behalf of the future of the American auto industry. to join me in supporting this bill.\n",
      "--------------------------------------------------\n",
      "Topic number:  1.0\n",
      "Topic contribution: 0.81\n",
      "Keywords: \n",
      " benefit,social_security,veterans,service,pay,elderly,employees,retire,receive,health\n",
      "Speech: \n",
      " The gentleman did not answer who pays for medicare. The fact of it is. part A is paid by the social security insurance program which older people paid into. Part B is paid by older people also. They pay $11 a month for medicare. and as a matter of fact. $11-\n",
      "--------------------------------------------------\n",
      "Topic number:  2.0\n",
      "Topic contribution: 0.97\n",
      "Keywords: \n",
      " go,say,think,amendment,get,want,gentleman,know,us,come\n",
      "Speech: \n",
      " I thank my colleague for yielding. Mr. Speaker. I would really like to speak to. the House. We are now. in my opinion. with this rule and this way of treating the minority. degrading this House. Yes. it is not funny. there is no joke here. We are tryIng to conduct the peoples business and we are degrading this House by playing politics with the rights of those of us in the minority. on this side. and of the people we represent. It is not funny. it is not a joke. It is a pity to see the Congress of the United States operating in this way.\n",
      "--------------------------------------------------\n",
      "Topic number:  3.0\n",
      "Topic contribution: 0.79\n",
      "Keywords: \n",
      " defense,military,arm,nuclear,force,security,aircraft,weapons,strategic,must\n",
      "Speech: \n",
      " Mr. Chairman I rise in support of the Broomfield substitute. The urgency of the arms control must not prevent our careful construction of verifiable force levels that retain our capability to deter potential aggressors. Reducing the number of weapons on both sides will reduce the risk of nuclear warfare. The Broomfield substitute will get us to acceptable. verifiable. flexible. and reduced force levels. Our commitment should be to the success of significant reductions before negotiating for a freeze on arms at existing and higher levels. Yet we must have the flexibility to modernize our forces. to redress technology gains by the Soviets so that we can provide a capable and lasting deterrent. No arms control agreement can. by itself. prevent or deter the development of new systems. We cannot put technology back on the shelf. but arms control agreements can reduce the levels of nuclear forces. We can provide for our national security within this regotiation of the reduction of levels. Mr. Chairman. I have spent a great deal of time on ths issue. I have met in Geneva with General Rowny. head of our START talks negotiating team. I have met with Mr. Nitze. the head of the INP negotiating team. I am convinced we must give them the flexibility available to meet their challenge. We have the new weapons systems such as the B1 and the MX going into our inventory. I have a son who Is flying a B25. That B25 was built in 1957. My son was built in 1955. We must look to action that promotes the longrange national security of the United States. We do not want to freeze nuclear weapons. we want to reduce them. But we want fair and equitable reductions that lead to greater stability. Supporting the Broomfield substitute gives us that flexibility.\n",
      "--------------------------------------------------\n",
      "Topic number:  4.0\n",
      "Topic contribution: 0.96\n",
      "Keywords: \n",
      " energy,project,cost,million,construction,job,need,study,percent,build\n",
      "Speech: \n",
      " We can run down them. if the Chairman would like. I did not know that that was tactful. But we have $41 million in Fort McPherson. $9 million in Fort Benning. $9 million in Fort Gordon: $20 million in Fort Sam Houston. $13 million in Fort Hood. $18 million In Aberdeen. $20 million in Mayport. $7 million in Whiting. $11 million in Eglin. and so on.\n",
      "--------------------------------------------------\n",
      "Topic number:  5.0\n",
      "Topic contribution: 0.97\n",
      "Keywords: \n",
      " school,serve,service,national,community,public,education,state,article,years\n",
      "Speech: \n",
      " Mr. Speaker. on September 12 I was honored to receive the LaGuardia Community College Presidents Medal for Community Service. The commencement address at that ceremony was delivered by Chancellor Joseph S. Murphy of the City University of New York. Chancellor Murphy gave what I consider to be an insightful and realistic speech on the policies of the Reagan administration. At this time. I would like to enclose this address in the REcoRD for my colleagues to read. Thank you.\n",
      "--------------------------------------------------\n",
      "Topic number:  6.0\n",
      "Topic contribution: 0.98\n",
      "Keywords: \n",
      " budget,tax,billion,cut,percent,increase,spend,year,program,federal\n",
      "Speech: \n",
      " Mr. Speaker. this is a continuation of my summaries of the Democratic Study Group report \"Reagans Deficit Disaster.\" (See H7191. September 20. 1982 and E4292. Let us look at some more facts. Set out below are the Federal surpluses and deficits since World War II. Note that deficit spending by administration offers some interesting statistics: Three of the four most deficitridden administrations are Republican: Reagan. Ford. and Nixon. Moreover. the only postWorld War II President to net out a surplus was the redoubtable Harry Truman. The record for deficits goes. of course. to the current occupant of the White House. Ronald Reagan.\n",
      "--------------------------------------------------\n",
      "Topic number:  7.0\n",
      "Topic contribution: 0.88\n",
      "Keywords: \n",
      " international,government,world,nations,resolution,american,policy,support,must,concern\n",
      "Speech: \n",
      " Mr. Speaker. World Habeas corpus. the Commission for International Due Process of Law has nominated MARIO BIAGGI for the Nobel Peace Prize. Our distinguished colleague was nominated for his laudatory efforts to insure peace in the north of Ireland. At this time. Mr. Speaker. I would like to insert in the RECORD an editorial which appeared in the February 13. 1982. Irish Echo.\n",
      "--------------------------------------------------\n",
      "Topic number:  8.0\n",
      "Topic contribution: 0.95\n",
      "Keywords: \n",
      " women,right,law,court,legal,state,case,constitution,congress,may\n",
      "Speech: \n",
      " Mr. Speaker. USA Today devoted its entire editorial page to the issue of pay equity for women. Its lead editorial is worthy of the attention of my colleagues. I ask that it be printed in the RECORD at this point. [From USA Today]\n",
      "--------------------------------------------------\n",
      "Topic number:  9.0\n",
      "Topic contribution: 0.96\n",
      "Keywords: \n",
      " committee,bill,amendment,house,act,provision,congress,legislation,vote,report\n",
      "Speech: \n",
      " Mr. Chairman. this paragraph contains a proviso stating that none of the funds made available for official expenses shall be considered as taxable to the President. This proviso is legislation in an appropriation bill and violates rule XXI. clause 2 of the Rules of the House of Representatives. In this case the paragraph provides compensation to the President of $250.000 for official expenses but states that none of the funds shall be taxable to the President. This Is a change in existing law or at least purports to be a change in existing law by stating the taxable status of the expenses. In effect It construes existing law and Is therefore not in order. (See CONGRESSIONAL RECORD. May 2. 195 1. pp. 47478.)\n",
      "--------------------------------------------------\n",
      "Topic number:  10.0\n",
      "Topic contribution: 0.95\n",
      "Keywords: \n",
      " program,provide,federal,support,bill,need,fund,legislation,act,work\n",
      "Speech: \n",
      " Mr. President. I am extremely pleased that the Senate is today considering legislation to address the plight of millions of Americans who suffer from rare diseases and conditions. Drugs for the treatment of these diseases are commonly know as orphan drugs due to the fact that high development and drug approval costs coupled with a small market for their use give these drugs limited commercial value. Thus. even in cases where a promising treatment has been discovered. it is frequently not made generally available because of the substantial financial loss which would be incurred by its manufacturer. I first became aware of this problem in 1981 when constituents visited my office to discuss the orphan drug problem as it relates to Huntingtons Disease. At that time. I introduced S. 1498 as an effort to facilitate the development of drugs for rare diseases. Based on suggestions which resulted from discussion of S. 1498. new orphan drug legislation was developed. I introduced this revised measure. S. 2130. earlier this year as a companion to H.R. 5238. The response to this legislation has been impressive. A bipartisan group of 27 Senators have joined in cosponsoring S. 2130. Cosponsors include Senators HATFIELD. JACKSON. METZENBAUM.\n",
      "--------------------------------------------------\n",
      "Topic number:  11.0\n",
      "Topic contribution: 0.99\n",
      "Keywords: \n",
      " years,us,great,live,work,many,day,know,world,today\n",
      "Speech: \n",
      " Mr. Speaker. as on each April 24. we honor the memory of the Armenians who suffered and perished in the massacres that began on this day in 1915. In honoring them. we also honor the roots of Armenian culture and the Armenian spirit. which spread throughout the world and which have made my own home of southern California much the richer for their vigor. Armenians throughout the world are joining together today in observance of the past and. through that. affirming their faith in the future. On April 24. 1915. we saw the start of the ultimate refinement of the limitless capacity of human crueltygenocide. Since that day. genocide has become part of a century of total war. total ideology. and total destruction. Its victims have become more numerous and more widespread with each decade. The Armenian people. as the first victims of this horror. stand as symbols for all that depravity can attempt. Yet. with the spirit of Armenia still blooming. still strong. these many years after those tragic events. I see the Armenian people also as showing us that strength can be drawn from the past. even a tragic past. The events of 1915 have brought home to every Armenian the faceless impersonal horror of the modern world. and yet they have found. within themselves and in their community. the strength not only to endure. but to excell. Today. while we honor the dead. there is no finer memorial to them than that which exists in the hearts and minds of our ArmenianAmerican friends and neighbors. As long as this spirit lives and thrives. which it does today. genocide has failed in its attempt to kill a people. Today. we honor the victims of 1915. we honor ArmenianAmericans. and we. through them. honor the spirit that has been kept burning even in the darkest moments of history. Armenia may not exist as a free country in the maps of the world. but it surely does in the hearts of its people.e\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(top_speeches_df.shape[0]):\n",
    "    print(\"Topic number: \", top_speeches_df.Prim_Topic[i])\n",
    "    print(\"Topic contribution: {:.2f}\".format(top_speeches_df.Prim_Topic_Contrib[i]))\n",
    "    print(\"Keywords: \\n\", top_speeches_df.Prim_Topic_Keywords[i])\n",
    "    print(\"Speech: \\n\", main_data[main_ids.index(top_speeches_df.Speech_id[i])])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary and secondary topic distribution in the speech file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(index=range(num_topics))\n",
    "# Number of Documents for Each Topic\n",
    "prim_topic_counts = all_speeches_topics_df['Prim_Topic'].value_counts().sort_index()\n",
    "sec_topic_counts = all_speeches_topics_df['Sec_Topic'].value_counts().sort_index()\n",
    "# Percentage of Documents for Each Topic\n",
    "prim_topic_share = round(prim_topic_counts/len(main_data), 2)\n",
    "prim_topic_share.name = 'Prim_Topic_Contr'\n",
    "sec_topic_share = round(sec_topic_counts/len(main_data), 2)\n",
    "sec_topic_share.name = 'Sec_Topic_Contr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3244, 1.0, 3193, 0.9900000000000001)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df= topics_df.join(prim_topic_counts)\\\n",
    "                    .join(prim_topic_share)\\\n",
    "                    .join(sec_topic_counts)\\\n",
    "                    .join(sec_topic_share)\\\n",
    "                    .join(pd.DataFrame.from_dict(keywords_dict, orient='index'))\n",
    "topics_df.reset_index(inplace=True)\n",
    "topics_df.columns = ['Topic_Num', \"Prim_Cnt\", \"Prim_Share\", \"Sec_Cnt\", \"Sec_Share\"] + ['kw'+str(i) for i in range(num_words)]\n",
    "topics_df.Prim_Cnt.sum(), topics_df.Prim_Share.sum(), topics_df.Sec_Cnt.sum(), topics_df.Sec_Share.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Prim_Cnt</th>\n",
       "      <th>Prim_Share</th>\n",
       "      <th>Sec_Cnt</th>\n",
       "      <th>Sec_Share</th>\n",
       "      <th>kw0</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw3</th>\n",
       "      <th>kw4</th>\n",
       "      <th>kw5</th>\n",
       "      <th>kw6</th>\n",
       "      <th>kw7</th>\n",
       "      <th>kw8</th>\n",
       "      <th>kw9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>852</td>\n",
       "      <td>0.26</td>\n",
       "      <td>494</td>\n",
       "      <td>0.15</td>\n",
       "      <td>go</td>\n",
       "      <td>say</td>\n",
       "      <td>think</td>\n",
       "      <td>amendment</td>\n",
       "      <td>get</td>\n",
       "      <td>want</td>\n",
       "      <td>gentleman</td>\n",
       "      <td>know</td>\n",
       "      <td>us</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>570</td>\n",
       "      <td>0.18</td>\n",
       "      <td>477</td>\n",
       "      <td>0.15</td>\n",
       "      <td>committee</td>\n",
       "      <td>bill</td>\n",
       "      <td>amendment</td>\n",
       "      <td>house</td>\n",
       "      <td>act</td>\n",
       "      <td>provision</td>\n",
       "      <td>congress</td>\n",
       "      <td>legislation</td>\n",
       "      <td>vote</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>430</td>\n",
       "      <td>0.13</td>\n",
       "      <td>327</td>\n",
       "      <td>0.10</td>\n",
       "      <td>years</td>\n",
       "      <td>us</td>\n",
       "      <td>great</td>\n",
       "      <td>live</td>\n",
       "      <td>work</td>\n",
       "      <td>many</td>\n",
       "      <td>day</td>\n",
       "      <td>know</td>\n",
       "      <td>world</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>349</td>\n",
       "      <td>0.11</td>\n",
       "      <td>352</td>\n",
       "      <td>0.11</td>\n",
       "      <td>program</td>\n",
       "      <td>provide</td>\n",
       "      <td>federal</td>\n",
       "      <td>support</td>\n",
       "      <td>bill</td>\n",
       "      <td>need</td>\n",
       "      <td>fund</td>\n",
       "      <td>legislation</td>\n",
       "      <td>act</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>280</td>\n",
       "      <td>0.09</td>\n",
       "      <td>269</td>\n",
       "      <td>0.08</td>\n",
       "      <td>budget</td>\n",
       "      <td>tax</td>\n",
       "      <td>billion</td>\n",
       "      <td>cut</td>\n",
       "      <td>percent</td>\n",
       "      <td>increase</td>\n",
       "      <td>spend</td>\n",
       "      <td>year</td>\n",
       "      <td>program</td>\n",
       "      <td>federal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>185</td>\n",
       "      <td>0.06</td>\n",
       "      <td>252</td>\n",
       "      <td>0.08</td>\n",
       "      <td>school</td>\n",
       "      <td>serve</td>\n",
       "      <td>service</td>\n",
       "      <td>national</td>\n",
       "      <td>community</td>\n",
       "      <td>public</td>\n",
       "      <td>education</td>\n",
       "      <td>state</td>\n",
       "      <td>article</td>\n",
       "      <td>years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>176</td>\n",
       "      <td>0.05</td>\n",
       "      <td>332</td>\n",
       "      <td>0.10</td>\n",
       "      <td>international</td>\n",
       "      <td>government</td>\n",
       "      <td>world</td>\n",
       "      <td>nations</td>\n",
       "      <td>resolution</td>\n",
       "      <td>american</td>\n",
       "      <td>policy</td>\n",
       "      <td>support</td>\n",
       "      <td>must</td>\n",
       "      <td>concern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>0.04</td>\n",
       "      <td>149</td>\n",
       "      <td>0.05</td>\n",
       "      <td>price</td>\n",
       "      <td>market</td>\n",
       "      <td>company</td>\n",
       "      <td>percent</td>\n",
       "      <td>industry</td>\n",
       "      <td>interest</td>\n",
       "      <td>american</td>\n",
       "      <td>cost</td>\n",
       "      <td>farm</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>0.03</td>\n",
       "      <td>151</td>\n",
       "      <td>0.05</td>\n",
       "      <td>energy</td>\n",
       "      <td>project</td>\n",
       "      <td>cost</td>\n",
       "      <td>million</td>\n",
       "      <td>construction</td>\n",
       "      <td>job</td>\n",
       "      <td>need</td>\n",
       "      <td>study</td>\n",
       "      <td>percent</td>\n",
       "      <td>build</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>0.02</td>\n",
       "      <td>140</td>\n",
       "      <td>0.04</td>\n",
       "      <td>defense</td>\n",
       "      <td>military</td>\n",
       "      <td>arm</td>\n",
       "      <td>nuclear</td>\n",
       "      <td>force</td>\n",
       "      <td>security</td>\n",
       "      <td>aircraft</td>\n",
       "      <td>weapons</td>\n",
       "      <td>strategic</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>74</td>\n",
       "      <td>0.02</td>\n",
       "      <td>136</td>\n",
       "      <td>0.04</td>\n",
       "      <td>women</td>\n",
       "      <td>right</td>\n",
       "      <td>law</td>\n",
       "      <td>court</td>\n",
       "      <td>legal</td>\n",
       "      <td>state</td>\n",
       "      <td>case</td>\n",
       "      <td>constitution</td>\n",
       "      <td>congress</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.01</td>\n",
       "      <td>114</td>\n",
       "      <td>0.04</td>\n",
       "      <td>benefit</td>\n",
       "      <td>social_security</td>\n",
       "      <td>veterans</td>\n",
       "      <td>service</td>\n",
       "      <td>pay</td>\n",
       "      <td>elderly</td>\n",
       "      <td>employees</td>\n",
       "      <td>retire</td>\n",
       "      <td>receive</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Num  Prim_Cnt  Prim_Share  Sec_Cnt  Sec_Share            kw0  \\\n",
       "2           2       852        0.26      494       0.15             go   \n",
       "9           9       570        0.18      477       0.15      committee   \n",
       "11         11       430        0.13      327       0.10          years   \n",
       "10         10       349        0.11      352       0.11        program   \n",
       "6           6       280        0.09      269       0.08         budget   \n",
       "5           5       185        0.06      252       0.08         school   \n",
       "7           7       176        0.05      332       0.10  international   \n",
       "0           0       126        0.04      149       0.05          price   \n",
       "4           4        82        0.03      151       0.05         energy   \n",
       "3           3        81        0.02      140       0.04        defense   \n",
       "8           8        74        0.02      136       0.04          women   \n",
       "1           1        39        0.01      114       0.04        benefit   \n",
       "\n",
       "                kw1        kw2        kw3           kw4        kw5        kw6  \\\n",
       "2               say      think  amendment           get       want  gentleman   \n",
       "9              bill  amendment      house           act  provision   congress   \n",
       "11               us      great       live          work       many        day   \n",
       "10          provide    federal    support          bill       need       fund   \n",
       "6               tax    billion        cut       percent   increase      spend   \n",
       "5             serve    service   national     community     public  education   \n",
       "7        government      world    nations    resolution   american     policy   \n",
       "0            market    company    percent      industry   interest   american   \n",
       "4           project       cost    million  construction        job       need   \n",
       "3          military        arm    nuclear         force   security   aircraft   \n",
       "8             right        law      court         legal      state       case   \n",
       "1   social_security   veterans    service           pay    elderly  employees   \n",
       "\n",
       "             kw7        kw8      kw9  \n",
       "2           know         us     come  \n",
       "9    legislation       vote   report  \n",
       "11          know      world    today  \n",
       "10   legislation        act     work  \n",
       "6           year    program  federal  \n",
       "5          state    article    years  \n",
       "7        support       must  concern  \n",
       "0           cost       farm    trade  \n",
       "4          study    percent    build  \n",
       "3        weapons  strategic     must  \n",
       "8   constitution   congress      may  \n",
       "1         retire    receive   health  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df.sort_values(by='Prim_Cnt', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
